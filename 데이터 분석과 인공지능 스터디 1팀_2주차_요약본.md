## [데이터 분석과 인공지능 스터디 1팀] 2주차 요약본

[👉스터디용 노션(공부 내용 공유용) 바로가기](https://gigantic-increase-961.notion.site/1-42d23dd397ce468d9aa4d24fcf7dc916)

### 방학~2주차 강의 리스트
- 파이썬 실전 데이터 분석
- 실습으로 배우는 NumPy/pandas
- 비전공자를 위한 머신러닝
- 프로그래밍 수학


### 스터디 일정
|       날짜       |         장소      |  참가인원 | 스터디 내용 |
| ---------------- | ----------------- | ------ | ------------- |
| 2022.01.30 21:30 | 디스코드 음성채널  |  5명 | [파이썬 실전 데이터 분석](#파이썬-실전-데이터-분석) |
| 2022.02.06 21:30 | 디스코드 음성채널  | 5명 | [Numpy, Pandas](#Numpy와-Pandas) | 
| 2022.02.10 21:30 |   엘리스 플랫폼  | 5명 | 머신러닝, 프로그래밍 수학 | 


### 파이썬 실전 데이터 분석

#### csv와 파이썬 함수들

(작성자 : 백지유)



![image](https://user-images.githubusercontent.com/71163016/153526193-688ccebd-317a-475d-a578-be843133572c.png)


#### 트럼프 대통령 트윗 분석
(작성자 : 김별희)
- 'foxandfriends:' , 'foxandfriends' 
- 'realDonaldTrump:', 'realDonaldTrump'
**⇒ @,# 제외한 특수기호 제거 하는 ` 전처리 과정 `이 필요함**
```python
from string import punctuation

print(punctuation)
>>> !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~

# '#'와 '@' 제외
punctuation = punctuation.replace('@',"").replace("#","")

preprocessed_texts = []
for text in texts:
  temp = text[1]
  for punc in punctuation:
    temp = temp.replace(punc, "")
  preprocessed_texts.append(temp.split())
  
clean_words = []

for text in preprocessed_texts:
  for word in text:
    if word not in stop_words:
      clean_words.append(word)
```

#### 영어 단어 모음으로 시작하는 텍스트 파일 분석
(작성자 : 박수정)
- 텍스트 파일 여러 개를 한 번에 불러온 후, `(단어, 빈도수)` 꼴의 튜플로 구성된 리스트를 반환합니다.
즉, 텍스트 파일을 읽어들여 튜플 꼴의 리스트 형태로 만드는 함수 입니다.
```python
def create_corpus(filenames):
    # 단어를 저장할 리스트를 생성합니다.
    words = []
    
    # 여러 파일에 등장하는 모든 단어를 모두 words에 저장합니다.
    for one_file in filenames:
        with open(one_file) as file:
            content=file.read()
        
            # 이 때 문장부호를 포함한 모든 특수기호를 제거합니다. 4번째 줄에서 임포트한 punctuation을  이용하세요.
            for symbol in punctuation:
                content = content.replace(symbol, '')
            words += content.split()
    
    # words 리스트의 데이터를 corpus 형태로 변환합니다. Counter() 사용 방법을 검색해보세요.
		# Counter() : from collections import Counter -> 각 단어의 갯수를 딕셔너리 형태로 반환
    corpus = Counter(words)
    return list(corpus.items())
```

#### 넷플릭스 시청 데이터로 알아보는 데이터형 변환
(작성자 : 천준석)
- 목적 :  작품 1과 사용자 A가 주어졌을 때, 예상 선호도를 계산합니다.
작품 1에 대한 사용자 A의 예상 선호도는 사용자 A가 시청한 모든 작품과 작품 1 유사도의 평균값입니다.

예를 들어, 사용자 A가 시청한 3개의 작품과 작품 1의 유사도가 0.6, 0.4, 0.5일 때, 선호도 점수는 0.5입니다.

```python
def predict_preference(title_to_users, user_to_titles, user, title):
    # user_to_titles를 이용해 user가 시청한 영화를 저장한다.
    titles = user_to_titles[user] # 해당 사용자가 시청한 작품들을 titles에 저장한다.
    # get_closeness() 함수를 이용해 유사도를 계산한다.
    closeness = []
    
    for my in titles: # 작품을 하나씩 꺼내면서
        closeness.append(get_closeness(title_to_users, my, title)) # 내가 시청한 작품과 특정 작품의 유사도를 계산하여 closeness 리스트에 넣는다

    return sum(closeness) / len(closeness) # 평균값 계산
```


### Numpy와 Pandas
(작성자 : 박수정)
- Matplotlib 데이터 시각화
```python
x = np.arange(10)
fig, ax = plt.subplots()
ax.plot(
    x, x, label='y=x',
    marker='o',
    color='blue',
    linestyle=':'
)
ax.plot(
    x, x**2, label='y=x^2',
    marker='^',
    color='red',
    linestyle='--' # - : 실선, -- : 선이 띄엄띄엄 연결, -, : 선과 점 연결, : : 점선
)
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.legend(
    loc='upper left',
    shadow=True,
    fancybox=True,
    borderpad=2
)

fig.savefig("plot.png")
```
![image](https://user-images.githubusercontent.com/71163016/153526865-472959ad-9509-4917-a3ab-63a915bcbd2a.png)
